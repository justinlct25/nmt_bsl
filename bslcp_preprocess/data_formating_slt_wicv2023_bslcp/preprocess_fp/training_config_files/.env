EXPERIMENT: baseline_6_3_dp03_wd_2

# task how2sign:train_sentencepiece_lowercased, for training the corresponding sentencepiece model
FAIRSEQ_ROOT: /content/drive/MyDrive/Git-repositories/slt_bslcp_wicv2023/
SAVE_DIR: /content/drive/MyDrive/Datasets/bslcp/dataset_preprocess_output
VOCAB_SIZE: 7000
FEATS: i3d
PARTITION: train

# task train_slt, for training the slt model
DATA_DIR: /content/drive/MyDrive/Datasets/bslcp/dataset_preprocess_output
WANDB_ENTITY: tinml
WANDB_PROJECT: wicv2023_replication_bslcp
NUM_GPUS: 1
CONFIG_DIR: /content/drive/MyDrive/Git-repositories/slt_bslcp_wicv2023/examples/sign_language/config/wicv_cvpr23/i3d_best
UPDATE_FREQ: 1

# others
I3D_DIR: /content/drive/MyDrive/Datasets/bslcp/dataset_preprocess_output
# WANDB_NAME: baseline_6_3_dp03_wd_2

# task generate, for evaluating the slt model
CKPT: checkpoint.best_reduced_sacrebleu_0.3500.pt
SUBSET: i3d.test.bslcp
SPM_MODEL: /content/drive/MyDrive/Datasets/bslcp/dataset_preprocess_output/vocab/cvpr23.train.how2sign.unigram7000_lowercased.model

